{% extends "base.html" %}

{% block title %}Corrección Ortográfica y Embeddings de Razas de Perros{% endblock %}

{% block content %}
<h2>Embeddings con Bigramas</h2>

<div class="motivation-container">
    <!-- Parte superior izquierda: Motivación -->
    <div class="motivation-left">
        <h2>Motivación</h2>
        <p>Este proyecto ocupa un lugar muy especial para mí, ya que fue el primer gran desafío que desarrollé completamente desde cero y llevé hasta su implementación en producción. Mientras trabajaba en una empresa de suplementos alimenticios, se me presentó un problema crucial: el buscador de productos de la empresa no estaba funcionando como debería. La empresa manejaba una amplia variedad de productos con nombres complejos y poco convencionales, lo que hacía que los usuarios a menudo no encontraran lo que buscaban debido a simples errores tipográficos o variaciones mínimas en la escritura de los nombres.
            Ante esta situación, se me encomendó la tarea de encontrar una solución que permitiera corregir estos errores y mejorar la precisión del buscador. Mi primera idea fue recurrir a alguna librería ya existente de corrección ortográfica. Sin embargo, muy pronto descubrí que estas herramientas estaban diseñadas para palabras comunes y corrientes, no para los nombres complejos y poco convencionales de los productos de la empresa. Este obstáculo me impulsó a idear un enfoque completamente nuevo y personalizado.
            Así fue como nació la idea de desarrollar un corrector ortográfico basado en embeddings, adaptado a las necesidades específicas de la empresa. Utilicé la base de datos de productos como referencia para generar un sistema capaz de corregir cualquier variación tipográfica de los nombres.</p>
    </div>

    <!-- Parte superior derecha: Imagen relacionada con el problema -->
    <div class="motivation-right-top">
        <img src="/static/images/suplementos.webp" alt="Imagen relacionada con la motivación">
    </div>

    <!-- Parte inferior izquierda: Línea de pensamiento y proceso creativo -->
    <div class="thoughts-creative-left">
        <h3>Línea de Pensamiento</h3>
        <p>El reto inicial fue entender cómo funciona un corrector ortográfico, pero pronto me di cuenta de que, en nuestro caso específico, no era relevante considerar el contexto de las oraciones. Lo esencial aquí era medir qué tan similares eran dos palabras. Me pregunté: ¿cómo encontramos una métrica adecuada para este tipo de comparación? 
            Tomemos como ejemplo la palabra botella. Si alguien la escribe como boteya, el error ortográfico recae solo en ciertas letras. El resto de la palabra sigue siendo correcto. Así, podríamos representarla como bote_a, donde el guion bajo indica que esa letra podría ser sustituida por varias opciones como ll o y. Me di cuenta de que muchas faltas ortográficas responden a patrones comunes: cambiar ll por y, c por s, u omitir letras. Entonces, parecía lógico diseñar un corrector que identificara estos patrones y los aplicara para sugerir correcciones.
            Sin embargo, había nombres de productos muy complejos en la base de datos, como ashwagandha, lo que hacía esta estrategia ineficaz. Fue entonces cuando pensé: ¿qué tal si dividimos las palabras? Al dividir boteya en fragmentos como bo, ot, te, _a, y hacer lo mismo con botella—bo, ot, te, ll, a—podemos notar que ambas comparten varias secuencias. Este método de fragmentación en pares de letras, conocido como bigrama, podría ser la clave. Con esta técnica, era posible comparar palabras y encontrar las que tuvieran mayor cantidad de bigramas en común, para luego devolver las mejores coincidencias.
            Así fue como implementé el sistema, pero no me detuve ahí. Para optimizar el proceso, agregué una serie de pasos. Primero, el sistema busca coincidencias exactas. Si no las encuentra, busca palabras que empiecen igual, y solo en último lugar ejecuta la comparación por bigramas para acelerar la consulta y mantener la eficiencia.</p>
            <p>En medio de este desarrollo, recordé algo que siempre me había fascinado: los embeddings de word2vec, donde las palabras se posicionan en un espacio vectorial dependiendo de su contexto. Palabras como computadora estarían más cerca de teclado que de cielo. Este modelo me inspiró, pero aún necesitaba una métrica adecuada para aplicar en nuestro contexto. La había encontrado en los bigramas.
            Con esta métrica, pude trasladar los nombres de productos a un espacio tridimensional, donde la cercanía entre palabras reflejaba su similitud. Aunque no podía utilizar los datos reales de la empresa, creé una demostración con razas de perros. Aunque menos complejo que los nombres de suplementos alimenticios, esta demostración captura la esencia de mi enfoque, mostrando el poder de los embeddings y las correcciones ortográficas personalizadas.</p>
    </div>

    <!-- Parte inferior derecha: Imagen relacionada con el proceso creativo -->
    <div class="motivation-right-bottom">
        <img src="/static/images/creativo.webp" alt="Imagen relacionada con el proceso creativo">
    </div>
</div>

<div class="container">
    <!-- Sección de introducción y explicación del algoritmo -->
    <h2>Corrección Ortográfica con Algoritmo de Bigrams y Embeddings</h2>
    <p>
        Utilizamos un sistema basado en <strong>bigrams</strong> y <strong>embeddings</strong> que permite corregir errores ortográficos comunes y manejar los nombres complejos de los productos. Este enfoque mejora la precisión del buscador, permitiendo sugerencias basadas en similitudes entre palabras.
    </p>


    <p style="text-align: center;">¿Interesado en más detalles técnicos? Aquí está el algoritmo que hace esto posible:</p>


    <h2>Ejemplo de Algoritmo con Bigrams</h2>
    <!-- Contenedor centrado para el bloque de código -->
        <div class="code-block">
            <pre>
        def generar_bigrams(palabra):
            bigrams = [palabra[i:i+2] for i in range(len(palabra) - 1)]
            return bigrams

        def generar_vector_bigramas(palabra, diccionario_bigramas):
            bigrams = generar_bigrams(palabra)
            vector = [0] * len[diccionario_bigramas]

            for bigrama in bigrams:
                if bigrama en diccionario_bigramas:
                    vector[diccionario_bigramas[bigrama]] = 1

            return vector
                    </pre>
    </div>

</div>
<div class="parallaxdog"></div>

<div class="container">
<h2>Caso de Uso: Embeddings para Razas de Perros</h2>
<p>
    Apliquemos este concepto en un caso práctico utilizando razas de perros. Hemos generado embeddings para una lista de razas de perros, y cuando un usuario introduce una raza, el sistema compara la raza ingresada con los embeddings de otras razas para encontrar la más cercana. Esto es útil para corregir errores tipográficos, como cuando alguien escribe "Beagel" en lugar de "Beagle".
</p>


<!-- Gráfico y Punto más cercano -->
<div class="graph-and-form">
    <div id="graph" data-initial-data="{{ initial_data }}"></div>

    <div id="closestResult">
        <h2>Punto más cercano</h2>

        <!-- Formulario -->
        <form id="textForm">
            <label for="text_input">Introduce una raza de perro:</label>
            <input type="text" id="text_input" name="text_input" required>
            <button type="submit">Buscar</button>
        </form>

        <p id="closestPointInfo">Introduce una raza de perro para ver la raza más cercana y la distancia.</p>
    </div>
</div>
</div>

<!-- Tarjetas explicativas del uso de embeddings y bigrams con razas de perros -->
<div class="card-container">
    <!-- Tarjeta 1: ¿Qué son los Embeddings? -->
    <div class="card">
        <h2>¿Qué son los Embeddings?</h2>
        <p>
            Los embeddings son representaciones vectoriales de objetos. En este caso, estamos usando razas de perros como ejemplos. Cada raza de perro se convierte en un vector que captura su "significado" y su relación con otras razas de perros. Así, los embeddings nos permiten medir similitudes entre las razas.
        </p>
    </div>

    <!-- Tarjeta 2: Uso de Bigrams -->
    <div class="card">
        <h2>Uso de Bigrams en Razas de Perros</h2>
        <p>
            Los bigrams descomponen los nombres de las razas de perros en pares de letras consecutivas. Por ejemplo, la palabra "Beagle" se convierte en los bigrams: "Be", "ea", "ag", "gl", "le". Esto ayuda a detectar y corregir errores tipográficos como "Beagel".
        </p>
    </div>

    <!-- Tarjeta 3: Corrección Basada en Similitud -->
    <div class="card">
        <h2>Corrección Basada en Similitud</h2>
        <p>
            El algoritmo compara los embeddings de las razas de perros para encontrar la más cercana a la ingresada. Si la distancia entre dos embeddings es pequeña, es probable que la raza ingresada sea un error tipográfico de una raza existente.
        </p>
    </div>
</div>

<div class="logo-container" style="text-align: center; margin-top: 50px;">
    <img src="{{ url_for('static', filename='images/logo.png') }}" alt="Logo" class="footer-logo">
</div>

<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script>
    document.addEventListener('DOMContentLoaded', function () {
        let initialDataElement = document.getElementById('graph');
        let initialData = JSON.parse(initialDataElement.getAttribute('data-initial-data'));

        let layout = {
            title: 'Visualización de Embeddings',
            scene: {
                xaxis: { title: 'PC1' },
                yaxis: { title: 'PC2' },
                zaxis: { title: 'PC3' }
            }
        };

        Plotly.newPlot('graph', initialData, layout);

        const form = document.getElementById('textForm');
        const closestPointInfo = document.getElementById('closestPointInfo');

        form.addEventListener('submit', function (event) {
            event.preventDefault();
            const formData = new FormData(form);
            const searchedWord = formData.get('text_input');

            fetch("{{ url_for('visualize') }}", {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                const newPoint = {
                    x: [data.new_vector[0]],
                    y: [data.new_vector[1]],
                    z: [data.new_vector[2]],
                    mode: 'markers+text',
                    type: 'scatter3d',
                    marker: { size: 5, color: 'red' },
                    text: ['Nuevo: ' + searchedWord],
                    textposition: 'top center',
                    name: 'Nuevo punto'
                };

                const closestPoint = {
                    x: [initialData[0].x[data.closest_index]],
                    y: [initialData[0].y[data.closest_index]],
                    z: [initialData[0].z[data.closest_index]],
                    mode: 'markers+text',
                    type: 'scatter3d',
                    marker: { size: 5, color: 'green' },
                    text: ['Cercano: ' + data.closest_title],
                    textposition: 'top center',
                    name: 'Punto más cercano'
                };

                Plotly.addTraces('graph', [newPoint, closestPoint]);
                closestPointInfo.innerHTML = `La raza más cercana a <strong>${searchedWord}</strong> es <strong>${data.closest_title}</strong> a una distancia de <strong>${data.closest_distance.toFixed(2)}</strong>`;
            })
            .catch(error => console.error('Error:', error));
        });
    });
</script>
<script src="{{ url_for('static', filename='js/script.js') }}"></script>

{% endblock %}
